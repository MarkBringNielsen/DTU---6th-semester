{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'toolbox_02450'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-08988ec2845a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtoolbox_02450\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrlr_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtoolbox_02450\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_neural_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw_neural_net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'toolbox_02450'"
     ]
    }
   ],
   "source": [
    "# exercise Load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.pyplot import (figure, subplot, plot, xlabel, ylabel, \n",
    "                               xticks, yticks,legend,show, hist, ylim)\n",
    "from scipy.stats import zscore\n",
    "from scipy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from matplotlib.pylab import (semilogx, loglog, \n",
    "                           title, grid)\n",
    "\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import rlr_validate\n",
    "from toolbox_02450 import train_neural_net, draw_neural_net\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import train_neural_net, draw_neural_net\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "#######################################################\n",
    "### Load the Wine csv data using the Pandas library ###\n",
    "#######################################################\n",
    "filename = 'wine.csv'\n",
    "df = pd.read_csv(filename, header=None)\n",
    "\n",
    "# Pandas returns a dataframe, (df) which could be used for handling the data.\n",
    "# We will however convert the dataframe to numpy arrays for this course as \n",
    "# is also described in the table in the exercise\n",
    "raw_data = df.values\n",
    "\n",
    "# Notice that raw_data both contains the information we want to store in an array\n",
    "# X (the sepal and petal dimensions) and the information that we wish to store \n",
    "# in y (the class labels, that is the iris species).\n",
    "\n",
    "# We start by making the data matrix X by indexing into data.\n",
    "# We know that the attributes are stored in the four columns from inspecting \n",
    "# the file.\n",
    "cols = range(1, 14)  \n",
    "X = raw_data[:, cols]\n",
    "\n",
    "# We can extract the attribute names that came from the header of the csv\n",
    "#attributeNames = np.asarray(df.columns[cols])\n",
    "#print(attributeNames)\n",
    "\n",
    "# Before we can store the class index, we need to convert the strings that\n",
    "# specify the class of a given object to a numerical value. We start by \n",
    "# extracting the strings for each sample from the raw data loaded from the csv:\n",
    "classLabels = raw_data[:,0] # 0 takes the first column\n",
    "# Then determine which classes are in the data by finding the set of \n",
    "# unique class labels \n",
    "classNames = np.unique(classLabels)\n",
    "# We can assign each type of WIne class with a number by making a\n",
    "# Python dictionary as so:\n",
    "classDict = dict(zip(classNames,range(len(classNames))))\n",
    "# The function zip simply \"zips\" togetter the classNames with an integer,\n",
    "# like a zipper on a jacket. \n",
    "# For instance, you could zip a list ['A', 'B', 'C'] with ['D', 'E', 'F'] to\n",
    "# get the pairs ('A','D'), ('B', 'E'), and ('C', 'F'). \n",
    "# A Python dictionary is a data object that stores pairs of a key with a value. \n",
    "# This means that when you call a dictionary with a given key, you \n",
    "# get the stored corresponding value. Try highlighting classDict and press F9.\n",
    "# You'll see that the first (key, value)-pair is ('Iris-setosa', 0). \n",
    "# If you look up in the dictionary classDict with the value 'Iris-setosa', \n",
    "# you will get the value 0. Try it with classDict['Iris-setosa']\n",
    "\n",
    "# With the dictionary, we can look up each data objects class label (the string)\n",
    "# in the dictionary, and determine which numerical value that object is \n",
    "# assigned. This is the class index vector y:\n",
    "y = np.array([classDict[cl] for cl in classLabels])\n",
    "# In the above, we have used the concept of \"list comprehension\", which\n",
    "# is a compact way of performing some operations on a list or array.\n",
    "# You could read the line  \"For each class label (cl) in the array of \n",
    "# class labels (classLabels), use the class label (cl) as the key and look up\n",
    "# in the class dictionary (classDict). Store the result for each class label\n",
    "# as an element in a list (because of the brackets []). Finally, convert the \n",
    "# list to a numpy array\". \n",
    "# Try running this to get a feel for the operation: \n",
    "# list = [0,1,2]\n",
    "# new_list = [element+10 for element in list]\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "#Choose the regularisation problem lige this: Taken from 7_2_1. \n",
    "#X,y = X[:,:10], X[:,10:]\n",
    "y = X[:,0] #Alcohol\n",
    "X = X[:, 1:] #[2,4,9,10,12]]\n",
    "# Choose Alcohol = offset, Ash, Magnesium, Color Intensity, Hue, Proline. (1 = 0 + 3 + 5 + 10 + 11 + 13) (/Numbers are WITH offset)\n",
    "#####################\n",
    "\n",
    "# We can determine the number of data objects and number of attributes using \n",
    "# the shape of X\n",
    "N, M = X.shape\n",
    "\n",
    "# Finally, the last variable that we need to have the dataset in the \n",
    "# \"standard representation\" for the course, is the number of classes, C:\n",
    "C = len(classNames)\n",
    "\n",
    "\n",
    "# Add offset attribute\n",
    "X = np.concatenate((np.ones((X.shape[0],1)),X),1)\n",
    "#attributeNames = [u'Offset']+attributeNames\n",
    "M = M+1\n",
    "\n",
    "\n",
    "#attributeNames = ('Offset', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline')\n",
    "#attributeNames = ('Offset', 'Ash', 'Magnesium', 'Color intensity', 'Hue' 'Proline')\n",
    "attributeNames = ('Offset', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline')\n",
    "\n",
    "#Standalize the data to zero mean and standard deviation of 1\n",
    "X_standarized = zscore(X, ddof=1) #Do the standalization \n",
    "\n",
    "\n",
    "\n",
    "# Parameters for neural network classifier\n",
    "n_hidden_units_m1 = 1      # number of hidden units\n",
    "n_hidden_units_m2 = 30      # number of hidden units\n",
    "n_hidden_units_m3 = 60      # number of hidden units\n",
    "n_hidden_units_m4 = 90\n",
    "n_replicates = 3        # number of networks trained in each k-fold\n",
    "max_iter = 10000\n",
    "number_of_ANN_modelse = 4;\n",
    "hidden_units_array = np.empty([number_of_ANN_modelse,1])\n",
    "hidden_units_array[0] = n_hidden_units_m1\n",
    "hidden_units_array[1] = n_hidden_units_m2\n",
    "hidden_units_array[2] = n_hidden_units_m3\n",
    "hidden_units_array[3] = n_hidden_units_m4\n",
    "\n",
    "\n",
    "# Define the models\n",
    "ANN_model_1 = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units_m1), #M features to n_hidden_units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units_m1, 1), # n_hidden_units to 1 output neuron\n",
    "                    # no final tranfer function, i.e. \"linear output\"\n",
    "                    )\n",
    "loss_fn_1 = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "\n",
    "ANN_model_2 = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units_m2), #M features to n_hidden_units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units_m2, 1), # n_hidden_units to 1 output neuron\n",
    "                    # no final tranfer function, i.e. \"linear output\"\n",
    "                    )\n",
    "loss_fn_2 = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "\n",
    "ANN_model_3 = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units_m3), #M features to n_hidden_units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units_m3, 1), # n_hidden_units to 1 output neuron\n",
    "                    # no final tranfer function, i.e. \"linear output\"\n",
    "                    )\n",
    "loss_fn_3 = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "\n",
    "ANN_model_4 = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units_m4), #M features to n_hidden_units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units_m4, 1), # n_hidden_units to 1 output neuron\n",
    "                    # no final tranfer function, i.e. \"linear output\"\n",
    "                    )\n",
    "loss_fn_4 = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "\n",
    "#ANN_error_1 = [] # make a list for storing generalizaition error in each loop\n",
    "#ANN_error_2 = [] # make a list for storing generalizaition error in each loop\n",
    "#ANN_error_3 = [] # make a list for storing generalizaition error in each loop\n",
    "\n",
    "############################################\n",
    "#Use exercise 8.1.1\n",
    "###########################################\n",
    "\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K1 = 5\n",
    "K2 = 5\n",
    "CV = model_selection.KFold(K1, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "#lambdas = np.power(10.,range(-5,9))\n",
    "lambdas = [0.0001,0.001, 0.01,0.1,1,10,20,30,40,100,1000,10000]\n",
    "#lambdas = np.power(10.,np.arange(-5,9,0.3))\n",
    "\n",
    "# Initialize variables\n",
    "#T = len(lambdas)\n",
    "Error_train = np.empty((K2,K1))\n",
    "Error_test = np.empty((K2,K1))\n",
    "Error_train_rlr = np.empty((K2,K1))\n",
    "Error_test_rlr = np.empty((K2,K1))\n",
    "Error_train_nofeatures = np.empty((K2,K1))\n",
    "Error_test_nofeatures = np.empty((K2,K1))\n",
    "w_rlr = np.empty((M,K1))\n",
    "#mu = np.empty((K1, M-1))\n",
    "#sigma = np.empty((K1, M-1))\n",
    "#w_noreg = np.empty((M,K1))\n",
    "\n",
    "\n",
    "ANN_error = np.empty([K2,number_of_ANN_modelse]) #10 is for crossvaidation and 3 is for ANN models\n",
    "ANN_best_error = np.empty([K2,1])\n",
    "optimal_h_array = np.empty((K2,1))\n",
    "\n",
    "\n",
    "optimal_lambda_array = np.empty((K2))\n",
    "dummy_optimal_lambda_array = np.empty((K2))\n",
    "\n",
    "Error_test_ANN = np.empty([K1])\n",
    "Error_test_lin_reg = np.empty([K1])\n",
    "Error_test_baseline = np.empty([K1])\n",
    "\n",
    "#For statistics\n",
    "CI_ab = np.empty([K1,2]) \n",
    "p_ab = np.empty([K1,1])\n",
    "CI_ac = np.empty([K1,2]) \n",
    "p_ac = np.empty([K1,1])\n",
    "CI_bc = np.empty([K1,2]) \n",
    "p_bc = np.empty([K1,1])\n",
    "\n",
    "k=0\n",
    "iii = 0\n",
    "for train_index_outer, test_index_outer in CV.split(X,y):\n",
    "    print('iii is:')\n",
    "    print(iii) \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X[train_index_outer]\n",
    "    y_train_outer = y[train_index_outer]\n",
    "    X_test_outer = X[test_index_outer]\n",
    "    y_test_outer = y[test_index_outer]\n",
    "        \n",
    "    for train_index_inner, test_index_inner in CV.split(X_train_outer,y_train_outer):\n",
    "        \n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X[train_index_inner]\n",
    "        y_train_inner = y[train_index_inner]\n",
    "        X_test_inner = X[test_index_inner]\n",
    "        y_test_inner = y[test_index_inner]\n",
    "        internal_cross_validation = 10    \n",
    "        \n",
    "        ######## \"The s for loop\" where each model is trained\n",
    "        opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train_inner, y_train_inner, lambdas, internal_cross_validation)\n",
    "        \n",
    "            # Extract training and test set for current CV fold, convert to tensors\n",
    "        X_train_ANN = torch.Tensor(X_train_inner) # X[train_index,:])\n",
    "        y_train_ANN = torch.Tensor(y_train_inner) # y[train_index])\n",
    "        X_test_ANN = torch.Tensor(X_test_inner) # X[test_index,:])\n",
    "        y_test_ANN = torch.Tensor(y_test_inner) # y[test_index])\n",
    "        \n",
    "        ###################\n",
    "        ### ANN model 1 ###\n",
    "        ###################\n",
    "        # Train the net on training data\n",
    "        net_m1, final_loss, learning_curve = train_neural_net(ANN_model_1,\n",
    "                                                           loss_fn_1,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M1 Best loss: {}\\n'.format(final_loss))\n",
    "        \n",
    "        # Determine estimated class labels for test set        \n",
    "        y_test_est_m1 = net_m1(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m1)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,0] = mse  # np.asarray(mse)\n",
    "       \n",
    "        \n",
    "        ### ANN model 2\n",
    "        # Train the net on training data\n",
    "        net_m2, final_loss, learning_curve = train_neural_net(ANN_model_2,\n",
    "                                                           loss_fn_2,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M2 Best loss: {}\\n'.format(final_loss))\n",
    "        \n",
    "       # Determine estimated class labels for test set        \n",
    "        y_test_est_m2 = net_m2(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m2)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,1] = mse  # np.asarray(mse)\n",
    "       \n",
    "        \n",
    "        ### ANN model 3\n",
    "        # Train the net on training data\n",
    "        net_m3, final_loss, learning_curve = train_neural_net(ANN_model_3,\n",
    "                                                           loss_fn_3,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M3 Best loss: {}\\n'.format(final_loss))\n",
    "         # Determine estimated class labels for test set        \n",
    "        y_test_est_m3 = net_m3(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m3)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,2] = mse  # np.asarray(mse)\n",
    "       \n",
    "        ### ANN model 4\n",
    "        # Train the net on training data\n",
    "        net_m4, final_loss, learning_curve = train_neural_net(ANN_model_4,\n",
    "                                                           loss_fn_4,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M4 Best loss: {}\\n'.format(final_loss))\n",
    "         # Determine estimated class labels for test set        \n",
    "        y_test_est_m4 = net_m4(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m4)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,3] = mse  # np.asarray(mse)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Standardize outer fold based on training set, and save the mean and standard\n",
    "        # deviations since they're part of the model (they would be needed for\n",
    "        # making new predictions) - for brevity we won't always store these in the scripts\n",
    "        #Uncommented these 4 lines (TK)\n",
    "        #mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "        #sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "        \n",
    "        #X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "        #X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "        \n",
    "        Xty = X_train_inner.T @ y_train_inner\n",
    "        XtX = X_train_inner.T @ X_train_inner\n",
    "        \n",
    "        # Compute mean squared error without using the input data at all - The base line model\n",
    "        Error_train_nofeatures[k] = np.square(y_train_inner-y_train_inner.mean()).sum(axis=0)/y_train_inner.shape[0]\n",
    "        Error_test_nofeatures[k] = np.square(y_test_inner-y_test_inner.mean()).sum(axis=0)/y_test_inner.shape[0]\n",
    "    \n",
    "        # Estimate weights for the optimal value of lambda, on entire training set\n",
    "        lambdaI = opt_lambda * np.eye(M)\n",
    "        lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "        w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "        # Compute mean squared error with regularization with optimal lambda\n",
    "        Error_train_rlr[k,iii] = np.square(y_train_inner-X_train_inner @ w_rlr[:,k]).sum(axis=0)/y_train_inner.shape[0]\n",
    "        Error_test_rlr[k,iii] = np.square(y_test_inner-X_test_inner @ w_rlr[:,k]).sum(axis=0)/y_test_inner.shape[0]\n",
    "        \n",
    "        dummy_optimal_lambda_array[k] = opt_lambda\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        k+=1\n",
    "    \n",
    "    #TK:\n",
    "    \n",
    "    #Pick out the best ANN model\n",
    "    index_dummy = np.where(ANN_error == np.amin(ANN_error)) #Minimun index, to safe h*\n",
    "    optimal_h_array[iii] = index_dummy[1]\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Extract training and test set for current CV fold, convert to tensors\n",
    "    X_train_outer_ANN = torch.Tensor(X_train_outer) # X[train_index,:])\n",
    "    y_train_outer_ANN = torch.Tensor(y_train_outer) # y[train_index])\n",
    "    X_test_outer_ANN = torch.Tensor(X_test_outer) # X[test_index,:])\n",
    "    y_test_outer_ANN = torch.Tensor(y_test_outer) # y[test_index])\n",
    "\n",
    "#Train the best model on the training set -- this could have been done in a nicer way... But python dont have switch case and I dont want to make a model array :p\n",
    "    if index_dummy[1] == 0:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_1,\n",
    "                                                           loss_fn_1,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M1 Best loss: {}\\n'.format(final_loss))\n",
    "    elif index_dummy[1] == 1:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_2,\n",
    "                                                           loss_fn_2,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M2 Best loss: {}\\n'.format(final_loss))\n",
    "    elif index_dummy[1] == 2:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_3,\n",
    "                                                           loss_fn_3,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M3 Best loss: {}\\n'.format(final_loss))\n",
    "    elif index_dummy[1] == 3:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_4,\n",
    "                                                           loss_fn_4,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M4 Best loss: {}\\n'.format(final_loss))\n",
    "    \n",
    "    # Determine estimated class labels for test set        \n",
    "    y_test_est_m_final = net_m_final(X_test_outer_ANN).detach().numpy() ### \n",
    "    # Determine Mean square error      \n",
    "    mse = np.square(y_test_outer-np.squeeze(y_test_est_m_final)).sum(axis=0)/y_test_outer.shape[0]\n",
    "    #Save the error\n",
    "    Error_test_ANN[iii] = mse #Minimum value \n",
    "    \n",
    "    #### Linear regression model\n",
    "    #Pick out the best optimal lambda value\n",
    "    min_lambda_index = np.where(Error_test_rlr[:,iii] == np.amin(Error_test_rlr[:,iii]))\n",
    "    optimal_lambda_array[iii] = dummy_optimal_lambda_array[min_lambda_index]\n",
    "    #Train the model:\n",
    "    Xty = X_train_outer.T @ y_train_outer\n",
    "    XtX = X_train_outer.T @ X_train_outer\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = optimal_lambda_array[iii] * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr_o = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    #Error_train_rlr[k,iii] = np.square(y_train_outer-X_train_outer@ w_rlr_o).sum(axis=0)/y_train_outer.shape[0]\n",
    "    Error_test_lin_reg[iii] = np.square(y_test_outer-X_test_outer @ w_rlr_o).sum(axis=0)/y_test_outer.shape[0]\n",
    "    \n",
    "    #### Base line model\n",
    "     # Compute mean squared error - The base line model\n",
    "    Error_test_baseline[iii] = np.square(y_test_outer-y_train_outer.mean()).sum(axis=0)/y_test_outer.shape[0]\n",
    "    \n",
    "    #Do the statistic evaluation. A is linear regression, B is ANN and C is baseline\n",
    "    yhatA = X_test_outer @ w_rlr_o\n",
    "    zA = np.abs(y_test_outer - yhatA ) ** 2\n",
    "    # yhatB = y_test_est_m_final\n",
    "    zB = np.abs(y_test_outer - np.squeeze(y_test_est_m_final) ) ** 2\n",
    "    # yhatC = y_train_outer.mean()\n",
    "    zC = np.abs(y_test_outer - y_train_outer.mean() ) ** 2\n",
    "    \n",
    "    alpha = 0.05\n",
    "    #Compare linear regression with ANN\n",
    "    z_ab = zA - zB\n",
    "    CI_ab[iii,:] = st.t.interval(1-alpha, len(z_ab)-1, loc=np.mean(z_ab), scale=st.sem(z_ab))  # Confidence interval\n",
    "    p_ab[iii] = st.t.cdf( -np.abs( np.mean(z_ab) )/st.sem(z_ab), df=len(z_ab)-1)  # p-value\n",
    "    #Compare linear regression with base line\n",
    "    z_ac = zA - zC\n",
    "    CI_ac[iii,:] = st.t.interval(1-alpha, len(z_ac)-1, loc=np.mean(z_ac), scale=st.sem(z_ac))  # Confidence interval\n",
    "    p_ac[iii] = st.t.cdf( -np.abs( np.mean(z_ac) )/st.sem(z_ac), df=len(z_ac)-1)  # p-value\n",
    "    #Compare ANN with base line\n",
    "    z_bc = zB - zC\n",
    "    CI_bc[iii,:] = st.t.interval(1-alpha, len(z_bc)-1, loc=np.mean(z_bc), scale=st.sem(z_bc))  # Confidence interval\n",
    "    p_ac[iii] = st.t.cdf( -np.abs( np.mean(z_bc) )/st.sem(z_bc), df=len(z_bc)-1)  # p-value\n",
    "  #  Error_test_ANN[iii]\n",
    "   # Error_test_lin_reg[iii]\n",
    "   # Error_test_baseline[iii]\n",
    "    \n",
    "    iii+=1\n",
    "    k = 0\n",
    "    \n",
    "    \n",
    "        # Estimate weights for unregularized linear regression, on entire training set\n",
    "        #w_noreg[:,k] = np.linalg.solve(XtX,Xty).squeeze()\n",
    "        # Compute mean squared error without regularization\n",
    "        #Error_train[k] = np.square(y_train-X_train @ w_noreg[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "        #Error_test[k] = np.square(y_test-X_test @ w_noreg[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "        # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "        #m = lm.LinearRegression().fit(X_train, y_train)\n",
    "        #Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "        #Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "        \n",
    "        \n",
    "#print(Error_test_rlr)        \n",
    "        \n",
    "\n",
    "\n",
    "print('Linear regression - Test error')\n",
    "#print(np.mean(Error_test_rlr,axis=0))\n",
    "print(Error_test_lin_reg)\n",
    "print('Optimal Lambda values)')\n",
    "print(optimal_lambda_array)   \n",
    "print('')\n",
    "print('Baseline')     \n",
    "print(Error_test_baseline)\n",
    "#print(np.mean(Error_test_nofeatures,axis=0))\n",
    "\n",
    "print('')\n",
    "print('ANN test error')     \n",
    "#print(ANN_best_error)\n",
    "print(Error_test_ANN)\n",
    "print('Optimal h values)')\n",
    "print(optimal_h_array)   \n",
    "\n",
    "print('Statistics')\n",
    "print('Compare linear regression with ANN')\n",
    "print(CI_ab)\n",
    "print(p_ab)\n",
    "print('Compare linear regression with baseline')\n",
    "print(CI_ac)\n",
    "print(p_ac)\n",
    "print('Compare ANN with baseline')\n",
    "print(CI_bc)\n",
    "print(p_bc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
