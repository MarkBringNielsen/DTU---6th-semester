{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wine.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "5   1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97  6.75  1.05  2.85   \n",
       "6   1  14.39  1.87  2.45  14.6   96  2.50  2.52  0.30  1.98  5.25  1.02  3.58   \n",
       "7   1  14.06  2.15  2.61  17.6  121  2.60  2.51  0.31  1.25  5.05  1.06  3.58   \n",
       "8   1  14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98  5.20  1.08  2.85   \n",
       "9   1  13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85  7.22  1.01  3.55   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  \n",
       "5  1450  \n",
       "6  1290  \n",
       "7  1295  \n",
       "8  1045  \n",
       "9  1045  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ebc5072908>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(0, axis=1)\n",
    "y = df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 1.710e+00, 2.430e+00, 1.560e+01, 1.270e+02, 2.800e+00,\n",
       "        3.060e+00, 2.800e-01, 2.290e+00, 5.640e+00, 1.040e+00, 3.920e+00,\n",
       "        1.065e+03],\n",
       "       [1.320e+01, 1.780e+00, 2.140e+00, 1.120e+01, 1.000e+02, 2.650e+00,\n",
       "        2.760e+00, 2.600e-01, 1.280e+00, 4.380e+00, 1.050e+00, 3.400e+00,\n",
       "        1.050e+03],\n",
       "       [1.316e+01, 2.360e+00, 2.670e+00, 1.860e+01, 1.010e+02, 2.800e+00,\n",
       "        3.240e+00, 3.000e-01, 2.810e+00, 5.680e+00, 1.030e+00, 3.170e+00,\n",
       "        1.185e+03],\n",
       "       [1.437e+01, 1.950e+00, 2.500e+00, 1.680e+01, 1.130e+02, 3.850e+00,\n",
       "        3.490e+00, 2.400e-01, 2.180e+00, 7.800e+00, 8.600e-01, 3.450e+00,\n",
       "        1.480e+03],\n",
       "       [1.324e+01, 2.590e+00, 2.870e+00, 2.100e+01, 1.180e+02, 2.800e+00,\n",
       "        2.690e+00, 3.900e-01, 1.820e+00, 4.320e+00, 1.040e+00, 2.930e+00,\n",
       "        7.350e+02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ebc561d3c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [y_train.value_counts().idxmax() for x in range(len(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        21\n",
      "           3       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n",
      "[[21  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class='multinomial')\n",
    "lr.fit(X_train, y_train)\n",
    "log_reg_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test, log_reg_pred))\n",
    "print(confusion_matrix(y_test, log_reg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of rfc is 0.9814814814814815\n",
      "Confusion Matrix of rfc is [[21  0  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0 12]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      0.95      0.98        21\n",
      "           3       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.97      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred= rfc.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of %s is %s\"%('rfc', acc))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix of %s is %s\"%('rfc', cm))\n",
    "print(f'\\n {classification_report(y_test, y_pred)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.39      1.00      0.56        21\n",
      "           3       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.39        54\n",
      "   macro avg       0.13      0.33      0.19        54\n",
      "weighted avg       0.15      0.39      0.22        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iii is:\n",
      "0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18\n",
      "  19  20  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  49  51  52  53  54  55  56  57  58\n",
      "  60  61  63  64  65  66  68  69  70  71  72  74  75  76  77  78  79  81\n",
      "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      " 100 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 136 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 151 152 155 156 157 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 176 177]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[0, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 176, 177] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-08349e794a65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1552\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1553\u001b[0m         )\n\u001b[0;32m   1554\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1643\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[0, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 176, 177] not in index'"
     ]
    }
   ],
   "source": [
    "filename = 'wine.csv'\n",
    "df = pd.read_csv(filename, header=None)\n",
    "\n",
    "\n",
    "X = df.drop(0, axis=1)\n",
    "y = df[0]\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K1 = 10\n",
    "K2 = 10\n",
    "CV = model_selection.KFold(n_splits=K1, shuffle=True)\n",
    "\n",
    "\n",
    "k=0\n",
    "iii = 0\n",
    "for train_index, test_index in CV.split(X):\n",
    "    print('iii is:')\n",
    "    print(iii) \n",
    "    \n",
    "    print (train_index)\n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    k_scores = []\n",
    "    for k in range(1, 21):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        scores = model_selection.cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "        k_scores.append(scores.mean())\n",
    "        print(k)\n",
    "        \n",
    "    print(k_scores.index(max(k_scores)), max(k_scores), ' : ', k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K1 = 5\n",
    "K2 = 5\n",
    "CV = model_selection.KFold(K1, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "#lambdas = np.power(10.,range(-5,9))\n",
    "lambdas = [0.0001,0.001, 0.01,0.1,1,10,20,30,40,100,1000,10000]\n",
    "#lambdas = np.power(10.,np.arange(-5,9,0.3))\n",
    "\n",
    "# Initialize variables\n",
    "#T = len(lambdas)\n",
    "Error_train = np.empty((K2,K1))\n",
    "Error_test = np.empty((K2,K1))\n",
    "Error_train_rlr = np.empty((K2,K1))\n",
    "Error_test_rlr = np.empty((K2,K1))\n",
    "Error_train_nofeatures = np.empty((K2,K1))\n",
    "Error_test_nofeatures = np.empty((K2,K1))\n",
    "w_rlr = np.empty((M,K1))\n",
    "#mu = np.empty((K1, M-1))\n",
    "#sigma = np.empty((K1, M-1))\n",
    "#w_noreg = np.empty((M,K1))\n",
    "\n",
    "\n",
    "ANN_error = np.empty([K2,number_of_ANN_modelse]) #10 is for crossvaidation and 3 is for ANN models\n",
    "ANN_best_error = np.empty([K2,1])\n",
    "optimal_h_array = np.empty((K2,1))\n",
    "\n",
    "\n",
    "optimal_lambda_array = np.empty((K2))\n",
    "dummy_optimal_lambda_array = np.empty((K2))\n",
    "\n",
    "Error_test_ANN = np.empty([K1])\n",
    "Error_test_lin_reg = np.empty([K1])\n",
    "Error_test_baseline = np.empty([K1])\n",
    "\n",
    "#For statistics\n",
    "CI_ab = np.empty([K1,2]) \n",
    "p_ab = np.empty([K1,1])\n",
    "CI_ac = np.empty([K1,2]) \n",
    "p_ac = np.empty([K1,1])\n",
    "CI_bc = np.empty([K1,2]) \n",
    "p_bc = np.empty([K1,1])\n",
    "\n",
    "k=0\n",
    "iii = 0\n",
    "for train_index_outer, test_index_outer in CV.split(X,y):\n",
    "    print('iii is:')\n",
    "    print(iii) \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X[train_index_outer]\n",
    "    y_train_outer = y[train_index_outer]\n",
    "    X_test_outer = X[test_index_outer]\n",
    "    y_test_outer = y[test_index_outer]\n",
    "        \n",
    "    for train_index_inner, test_index_inner in CV.split(X_train_outer,y_train_outer):\n",
    "        \n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X[train_index_inner]\n",
    "        y_train_inner = y[train_index_inner]\n",
    "        X_test_inner = X[test_index_inner]\n",
    "        y_test_inner = y[test_index_inner]\n",
    "        internal_cross_validation = 10    \n",
    "        \n",
    "        ######## \"The s for loop\" where each model is trained\n",
    "        opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train_inner, y_train_inner, lambdas, internal_cross_validation)\n",
    "        \n",
    "            # Extract training and test set for current CV fold, convert to tensors\n",
    "        X_train_ANN = torch.Tensor(X_train_inner) # X[train_index,:])\n",
    "        y_train_ANN = torch.Tensor(y_train_inner) # y[train_index])\n",
    "        X_test_ANN = torch.Tensor(X_test_inner) # X[test_index,:])\n",
    "        y_test_ANN = torch.Tensor(y_test_inner) # y[test_index])\n",
    "        \n",
    "        ###################\n",
    "        ### ANN model 1 ###\n",
    "        ###################\n",
    "        # Train the net on training data\n",
    "        net_m1, final_loss, learning_curve = train_neural_net(ANN_model_1,\n",
    "                                                           loss_fn_1,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M1 Best loss: {}\\n'.format(final_loss))\n",
    "        \n",
    "        # Determine estimated class labels for test set        \n",
    "        y_test_est_m1 = net_m1(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m1)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,0] = mse  # np.asarray(mse)\n",
    "       \n",
    "        \n",
    "        ### ANN model 2\n",
    "        # Train the net on training data\n",
    "        net_m2, final_loss, learning_curve = train_neural_net(ANN_model_2,\n",
    "                                                           loss_fn_2,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M2 Best loss: {}\\n'.format(final_loss))\n",
    "        \n",
    "       # Determine estimated class labels for test set        \n",
    "        y_test_est_m2 = net_m2(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m2)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,1] = mse  # np.asarray(mse)\n",
    "       \n",
    "        \n",
    "        ### ANN model 3\n",
    "        # Train the net on training data\n",
    "        net_m3, final_loss, learning_curve = train_neural_net(ANN_model_3,\n",
    "                                                           loss_fn_3,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M3 Best loss: {}\\n'.format(final_loss))\n",
    "         # Determine estimated class labels for test set        \n",
    "        y_test_est_m3 = net_m3(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m3)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,2] = mse  # np.asarray(mse)\n",
    "       \n",
    "        ### ANN model 4\n",
    "        # Train the net on training data\n",
    "        net_m4, final_loss, learning_curve = train_neural_net(ANN_model_4,\n",
    "                                                           loss_fn_4,\n",
    "                                                           X=X_train_ANN,\n",
    "                                                           y=y_train_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M4 Best loss: {}\\n'.format(final_loss))\n",
    "         # Determine estimated class labels for test set        \n",
    "        y_test_est_m4 = net_m4(X_test_ANN).detach().numpy() ### \n",
    "        # Determine Mean square error      \n",
    "        mse = np.square(y_test_inner-np.squeeze(y_test_est_m4)).sum(axis=0)/y_test_inner.shape[0]\n",
    "        # Save it\n",
    "        ANN_error[k,3] = mse  # np.asarray(mse)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Standardize outer fold based on training set, and save the mean and standard\n",
    "        # deviations since they're part of the model (they would be needed for\n",
    "        # making new predictions) - for brevity we won't always store these in the scripts\n",
    "        #Uncommented these 4 lines (TK)\n",
    "        #mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "        #sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "        \n",
    "        #X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "        #X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "        \n",
    "        Xty = X_train_inner.T @ y_train_inner\n",
    "        XtX = X_train_inner.T @ X_train_inner\n",
    "        \n",
    "        # Compute mean squared error without using the input data at all - The base line model\n",
    "        Error_train_nofeatures[k] = np.square(y_train_inner-y_train_inner.mean()).sum(axis=0)/y_train_inner.shape[0]\n",
    "        Error_test_nofeatures[k] = np.square(y_test_inner-y_test_inner.mean()).sum(axis=0)/y_test_inner.shape[0]\n",
    "    \n",
    "        # Estimate weights for the optimal value of lambda, on entire training set\n",
    "        lambdaI = opt_lambda * np.eye(M)\n",
    "        lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "        w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "        # Compute mean squared error with regularization with optimal lambda\n",
    "        Error_train_rlr[k,iii] = np.square(y_train_inner-X_train_inner @ w_rlr[:,k]).sum(axis=0)/y_train_inner.shape[0]\n",
    "        Error_test_rlr[k,iii] = np.square(y_test_inner-X_test_inner @ w_rlr[:,k]).sum(axis=0)/y_test_inner.shape[0]\n",
    "        \n",
    "        dummy_optimal_lambda_array[k] = opt_lambda\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        k+=1\n",
    "    \n",
    "    #TK:\n",
    "    \n",
    "    #Pick out the best ANN model\n",
    "    index_dummy = np.where(ANN_error == np.amin(ANN_error)) #Minimun index, to safe h*\n",
    "    optimal_h_array[iii] = index_dummy[1]\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Extract training and test set for current CV fold, convert to tensors\n",
    "    X_train_outer_ANN = torch.Tensor(X_train_outer) # X[train_index,:])\n",
    "    y_train_outer_ANN = torch.Tensor(y_train_outer) # y[train_index])\n",
    "    X_test_outer_ANN = torch.Tensor(X_test_outer) # X[test_index,:])\n",
    "    y_test_outer_ANN = torch.Tensor(y_test_outer) # y[test_index])\n",
    "\n",
    "#Train the best model on the training set -- this could have been done in a nicer way... But python dont have switch case and I dont want to make a model array :p\n",
    "    if index_dummy[1] == 0:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_1,\n",
    "                                                           loss_fn_1,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M1 Best loss: {}\\n'.format(final_loss))\n",
    "    elif index_dummy[1] == 1:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_2,\n",
    "                                                           loss_fn_2,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M2 Best loss: {}\\n'.format(final_loss))\n",
    "    elif index_dummy[1] == 2:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_3,\n",
    "                                                           loss_fn_3,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M3 Best loss: {}\\n'.format(final_loss))\n",
    "    elif index_dummy[1] == 3:\n",
    "        net_m_final, final_loss, learning_curve = train_neural_net(ANN_model_4,\n",
    "                                                           loss_fn_4,\n",
    "                                                           X=X_train_outer_ANN,\n",
    "                                                           y=y_train_outer_ANN,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\t M4 Best loss: {}\\n'.format(final_loss))\n",
    "    \n",
    "    # Determine estimated class labels for test set        \n",
    "    y_test_est_m_final = net_m_final(X_test_outer_ANN).detach().numpy() ### \n",
    "    # Determine Mean square error      \n",
    "    mse = np.square(y_test_outer-np.squeeze(y_test_est_m_final)).sum(axis=0)/y_test_outer.shape[0]\n",
    "    #Save the error\n",
    "    Error_test_ANN[iii] = mse #Minimum value \n",
    "    \n",
    "    #### Linear regression model\n",
    "    #Pick out the best optimal lambda value\n",
    "    min_lambda_index = np.where(Error_test_rlr[:,iii] == np.amin(Error_test_rlr[:,iii]))\n",
    "    optimal_lambda_array[iii] = dummy_optimal_lambda_array[min_lambda_index]\n",
    "    #Train the model:\n",
    "    Xty = X_train_outer.T @ y_train_outer\n",
    "    XtX = X_train_outer.T @ X_train_outer\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = optimal_lambda_array[iii] * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr_o = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    #Error_train_rlr[k,iii] = np.square(y_train_outer-X_train_outer@ w_rlr_o).sum(axis=0)/y_train_outer.shape[0]\n",
    "    Error_test_lin_reg[iii] = np.square(y_test_outer-X_test_outer @ w_rlr_o).sum(axis=0)/y_test_outer.shape[0]\n",
    "    \n",
    "    #### Base line model\n",
    "     # Compute mean squared error - The base line model\n",
    "    Error_test_baseline[iii] = np.square(y_test_outer-y_train_outer.mean()).sum(axis=0)/y_test_outer.shape[0]\n",
    "    \n",
    "    #Do the statistic evaluation. A is linear regression, B is ANN and C is baseline\n",
    "    yhatA = X_test_outer @ w_rlr_o\n",
    "    zA = np.abs(y_test_outer - yhatA ) ** 2\n",
    "    # yhatB = y_test_est_m_final\n",
    "    zB = np.abs(y_test_outer - np.squeeze(y_test_est_m_final) ) ** 2\n",
    "    # yhatC = y_train_outer.mean()\n",
    "    zC = np.abs(y_test_outer - y_train_outer.mean() ) ** 2\n",
    "    \n",
    "    alpha = 0.05\n",
    "    #Compare linear regression with ANN\n",
    "    z_ab = zA - zB\n",
    "    CI_ab[iii,:] = st.t.interval(1-alpha, len(z_ab)-1, loc=np.mean(z_ab), scale=st.sem(z_ab))  # Confidence interval\n",
    "    p_ab[iii] = st.t.cdf( -np.abs( np.mean(z_ab) )/st.sem(z_ab), df=len(z_ab)-1)  # p-value\n",
    "    #Compare linear regression with base line\n",
    "    z_ac = zA - zC\n",
    "    CI_ac[iii,:] = st.t.interval(1-alpha, len(z_ac)-1, loc=np.mean(z_ac), scale=st.sem(z_ac))  # Confidence interval\n",
    "    p_ac[iii] = st.t.cdf( -np.abs( np.mean(z_ac) )/st.sem(z_ac), df=len(z_ac)-1)  # p-value\n",
    "    #Compare ANN with base line\n",
    "    z_bc = zB - zC\n",
    "    CI_bc[iii,:] = st.t.interval(1-alpha, len(z_bc)-1, loc=np.mean(z_bc), scale=st.sem(z_bc))  # Confidence interval\n",
    "    p_ac[iii] = st.t.cdf( -np.abs( np.mean(z_bc) )/st.sem(z_bc), df=len(z_bc)-1)  # p-value\n",
    "  #  Error_test_ANN[iii]\n",
    "   # Error_test_lin_reg[iii]\n",
    "   # Error_test_baseline[iii]\n",
    "    \n",
    "    iii+=1\n",
    "    k = 0\n",
    "    \n",
    "    \n",
    "        # Estimate weights for unregularized linear regression, on entire training set\n",
    "        #w_noreg[:,k] = np.linalg.solve(XtX,Xty).squeeze()\n",
    "        # Compute mean squared error without regularization\n",
    "        #Error_train[k] = np.square(y_train-X_train @ w_noreg[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "        #Error_test[k] = np.square(y_test-X_test @ w_noreg[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "        # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "        #m = lm.LinearRegression().fit(X_train, y_train)\n",
    "        #Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "        #Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "        \n",
    "        \n",
    "#print(Error_test_rlr)        \n",
    "        \n",
    "\n",
    "\n",
    "print('Linear regression - Test error')\n",
    "#print(np.mean(Error_test_rlr,axis=0))\n",
    "print(Error_test_lin_reg)\n",
    "print('Optimal Lambda values)')\n",
    "print(optimal_lambda_array)   \n",
    "print('')\n",
    "print('Baseline')     \n",
    "print(Error_test_baseline)\n",
    "#print(np.mean(Error_test_nofeatures,axis=0))\n",
    "\n",
    "print('')\n",
    "print('ANN test error')     \n",
    "#print(ANN_best_error)\n",
    "print(Error_test_ANN)\n",
    "print('Optimal h values)')\n",
    "print(optimal_h_array)   \n",
    "\n",
    "print('Statistics')\n",
    "print('Compare linear regression with ANN')\n",
    "print(CI_ab)\n",
    "print(p_ab)\n",
    "print('Compare linear regression with baseline')\n",
    "print(CI_ac)\n",
    "print(p_ac)\n",
    "print('Compare ANN with baseline')\n",
    "print(CI_bc)\n",
    "print(p_bc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
